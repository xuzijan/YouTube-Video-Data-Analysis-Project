"""
YouTubeæ ‡é¢˜è´¨é‡æ‰“åˆ†å™¨
åŸºäºæ¢¯åº¦æå‡ + DeepSeek APIçš„åˆ›é€ åŠ›è¯„ä¼°
"""

import sys
import os
sys.stdout.reconfigure(encoding='utf-8')

import pandas as pd
import numpy as np
import re
import warnings
from datetime import datetime
import requests
import json
import time
from typing import Dict, List, Tuple

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import xgboost as xgb
import shap

warnings.filterwarnings('ignore')

# ==================== é…ç½® ====================
# ä»config.pyå¯¼å…¥é…ç½®
try:
    from config import DEEPSEEK_API_KEY, DEEPSEEK_API_URL, DATA_PATH
except ImportError:
    # å¦‚æœconfig.pyä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
    DEEPSEEK_API_KEY = "your_api_key_here"
    DEEPSEEK_API_URL = "https://api.deepseek.com/chat/completions"
    DATA_PATH = "youtube_video.csv"

# ==================== æ•°æ®åŠ è½½ä¸åˆæ­¥æ¢ç´¢ ====================

def load_data(filepath: str) -> pd.DataFrame:
    """åŠ è½½YouTubeè§†é¢‘æ•°æ®"""
    df = pd.read_csv(filepath)
    print(f"æ•°æ®åŠ è½½å®Œæˆ: {len(df)} æ¡è®°å½•")
    print(f"åˆ—ä¿¡æ¯: {df.columns.tolist()}")
    return df

def calculate_engagement_metrics(df: pd.DataFrame) -> pd.DataFrame:
    """è®¡ç®—äº’åŠ¨æŒ‡æ ‡"""
    df['view_count'] = pd.to_numeric(df['view_count'], errors='coerce')
    df['like_count'] = pd.to_numeric(df['like_count'], errors='coerce')
    df['comment_count'] = pd.to_numeric(df['comment_count'], errors='coerce')
    
    # äº’åŠ¨ç‡ï¼ˆå¤„ç†0å€¼ï¼‰
    df['engagement_rate'] = (df['like_count'] + df['comment_count']) / (df['view_count'] + 1)
    df['like_rate'] = df['like_count'] / (df['view_count'] + 1)
    df['comment_rate'] = df['comment_count'] / (df['view_count'] + 1)
    
    # å¯¹æ•°å˜æ¢ï¼ˆç¼“è§£å¼‚å¸¸å€¼ï¼‰
    df['log_engagement'] = np.log1p(df['engagement_rate'] * 1000)
    
    return df

# ==================== æ–‡æœ¬ç‰¹å¾å·¥ç¨‹ ====================

def extract_text_features(df: pd.DataFrame) -> pd.DataFrame:
    """ä»æ ‡é¢˜æå–æ–‡æœ¬ç‰¹å¾"""
    
    features = {
        'title_length': [],
        'title_word_count': [],
        'uppercase_ratio': [],
        'digit_count': [],
        'exclamation_count': [],
        'question_count': [],
        'parenthesis_count': [],
        'pipe_count': [],
        'dash_count': [],
        'emoji_count': [],
        'has_colon': [],
        'has_brand': [],
        'all_caps_words': [],
    }
    
    for title in df['title']:
        if pd.isna(title):
            title = ""
        
        # åŸºç¡€é•¿åº¦
        features['title_length'].append(len(title))
        features['title_word_count'].append(len(title.split()))
        
        # å¤§å†™æ¯”ä¾‹
        upper_chars = sum(1 for c in title if c.isupper())
        features['uppercase_ratio'].append(upper_chars / (len(title) + 1))
        
        # æ•°å­—
        features['digit_count'].append(len(re.findall(r'\d', title)))
        
        # ç‰¹æ®Šå­—ç¬¦ç»Ÿè®¡
        features['exclamation_count'].append(title.count('!'))
        features['question_count'].append(title.count('?'))
        features['parenthesis_count'].append(title.count('(') + title.count(')'))
        features['pipe_count'].append(title.count('|'))
        features['dash_count'].append(title.count('-'))
        
        # Emojiç»Ÿè®¡ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        emoji_pattern = re.compile("["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F700-\U0001F77F"  # alchemical symbols
            "\U0001F780-\U0001F7FF"  # Geometric Shapes Extended
            "\U0001F800-\U0001F8FF"  # Supplemental Arrows-C
            "\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
            "\U0001FA00-\U0001FA6F"  # Chess Symbols
            "\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "]+", flags=re.UNICODE)
        features['emoji_count'].append(len(emoji_pattern.findall(title)))
        
        # å†’å·å’Œå“ç‰Œæ ‡è®°
        features['has_colon'].append(1 if ':' in title else 0)
        features['has_brand'].append(1 if any(x in title.lower() for x in ['official', '#shorts', 'trailer']) else 0)
        
        # å…¨å¤§å†™è¯æ•°é‡
        all_caps = len([w for w in title.split() if w.isupper() and len(w) > 1])
        features['all_caps_words'].append(all_caps)
    
    feature_df = pd.DataFrame(features)
    print(f"æ–‡æœ¬ç‰¹å¾æå–å®Œæˆ: {len(feature_df.columns)} ä¸ªç‰¹å¾")
    
    return pd.concat([df, feature_df], axis=1)

# ==================== DeepSeek API è°ƒç”¨ ====================

def call_deepseek_api(title: str, aspect: str = "creativity") -> float:
    """
    è°ƒç”¨DeepSeek APIè¯„ä¼°æ ‡é¢˜çš„åˆ›é€ åŠ›/æ–°é¢–æ€§ç­‰ä¸»è§‚ç»´åº¦
    
    Args:
        title: è§†é¢‘æ ‡é¢˜
        aspect: è¯„ä¼°ç»´åº¦ ('creativity' / 'emotional_appeal' / 'clarity')
    
    Returns:
        è¯„åˆ† 0-10
    """
    
    prompts = {
        "creativity": f"""è¯·è¯„ä¼°ä»¥ä¸‹YouTubeè§†é¢‘æ ‡é¢˜çš„åˆ›é€ åŠ›å’Œæ–°é¢–æ€§ï¼Œè¿”å›0-10çš„åˆ†æ•°ï¼Œå…¶ä¸­10è¡¨ç¤ºæå…·åˆ›æ„ï¼š
æ ‡é¢˜: "{title}"
åªè¿”å›ä¸€ä¸ªæ•°å­—ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚""",
        
        "emotional_appeal": f"""è¯·è¯„ä¼°ä»¥ä¸‹YouTubeè§†é¢‘æ ‡é¢˜çš„æƒ…æ„Ÿå¸å¼•åŠ›å’Œå¥½å¥‡å¿ƒæ¿€å‘ç¨‹åº¦ï¼Œè¿”å›0-10çš„åˆ†æ•°ï¼š
æ ‡é¢˜: "{title}"
åªè¿”å›ä¸€ä¸ªæ•°å­—ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚""",
        
        "clarity": f"""è¯·è¯„ä¼°ä»¥ä¸‹YouTubeè§†é¢‘æ ‡é¢˜çš„æ¸…æ™°åº¦å’Œæ˜“ç†è§£ç¨‹åº¦ï¼Œè¿”å›0-10çš„åˆ†æ•°ï¼š
æ ‡é¢˜: "{title}"
åªè¿”å›ä¸€ä¸ªæ•°å­—ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""
    }
    
    prompt = prompts.get(aspect, prompts["creativity"])
    
    try:
        response = requests.post(
            DEEPSEEK_API_URL,
            headers={
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "model": "deepseek-chat",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.3,
                "max_tokens": 10
            },
            timeout=10
        )
        
        if response.status_code == 200:
            result = response.json()
            score_text = result['choices'][0]['message']['content'].strip()
            # æå–æ•°å­—
            score = float(re.findall(r'\d+\.?\d*', score_text)[0])
            return min(10, max(0, score))  # é™åˆ¶åœ¨0-10
        else:
            print(f"APIé”™è¯¯: {response.status_code}")
            return 5.0  # é»˜è®¤å€¼
    except Exception as e:
        print(f"APIè°ƒç”¨å¤±è´¥: {e}")
        return 5.0

def batch_deepseek_scoring(titles: List[str], aspect: str = "creativity", 
                           sample_size: int = None, delay: float = 0.5) -> Dict[str, float]:
    """
    æ‰¹é‡è¯„ä¼°æ ‡é¢˜ï¼ˆå¸¦é‡‡æ ·å’Œå»¶è¿Ÿæ§åˆ¶ï¼‰
    
    Args:
        titles: æ ‡é¢˜åˆ—è¡¨
        aspect: è¯„ä¼°ç»´åº¦
        sample_size: é‡‡æ ·æ•°é‡ï¼ˆä¸æŒ‡å®šåˆ™å…¨éƒ¨è¯„ä¼°ï¼‰
        delay: è¯·æ±‚é—´éš”ç§’æ•°
    
    Returns:
        æ ‡é¢˜->è¯„åˆ†å­—å…¸
    """
    
    if sample_size:
        indices = np.random.choice(len(titles), min(sample_size, len(titles)), replace=False)
        sampled_titles = [titles[i] for i in indices]
    else:
        sampled_titles = titles
    
    scores = {}
    print(f"\nå¼€å§‹DeepSeek APIè¯„ä¼° ({len(sampled_titles)}æ¡)...")
    
    for i, title in enumerate(sampled_titles):
        if pd.isna(title):
            scores[title] = 5.0
        else:
            score = call_deepseek_api(title, aspect)
            scores[title] = score
            
            if (i + 1) % 10 == 0:
                print(f"  å·²å®Œæˆ {i + 1}/{len(sampled_titles)}")
            
            time.sleep(delay)
    
    print(f"DeepSeekè¯„ä¼°å®Œæˆ")
    return scores

def add_deepseek_features(df: pd.DataFrame, sample_size: int = 1000) -> pd.DataFrame:
    """
    ä¸ºæ•°æ®é›†æ·»åŠ DeepSeekè¯„åˆ†ç‰¹å¾
    
    æ³¨æ„ï¼šéœ€è¦é…ç½®æœ‰æ•ˆçš„APIå¯†é’¥
    """
    
    print(f"\nDeepSeek APIé›†æˆæ¨¡å¼ (é‡‡æ ·{sample_size}æ¡è¿›è¡Œè¯„ä¼°)")
    print(f"   APIå¯†é’¥çŠ¶æ€: {'âœ“ å·²é…ç½®' if DEEPSEEK_API_KEY != 'your_api_key_here' else 'âœ— æœªé…ç½®'}")
    
    if DEEPSEEK_API_KEY == "your_api_key_here":
        print(" è·³è¿‡APIè°ƒç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®...")
        # æ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
        np.random.seed(42)
        df['creativity_score'] = np.random.uniform(4, 9, len(df))
        df['emotional_appeal_score'] = np.random.uniform(3, 9, len(df))
        df['clarity_score'] = np.random.uniform(5, 10, len(df))
    else:
        # çœŸå®APIè°ƒç”¨
        creativity_scores = batch_deepseek_scoring(
            df['title'].tolist(), 
            aspect="creativity", 
            sample_size=sample_size
        )
        
        emotional_scores = batch_deepseek_scoring(
            df['title'].tolist(), 
            aspect="emotional_appeal", 
            sample_size=sample_size
        )
        
        df['creativity_score'] = df['title'].map(
            lambda x: creativity_scores.get(x, 5.0)
        )
        df['emotional_appeal_score'] = df['title'].map(
            lambda x: emotional_scores.get(x, 5.0)
        )
        df['clarity_score'] = np.random.uniform(5, 10, len(df))  # å¤‡ç”¨
    
    print(f" DeepSeekç‰¹å¾æ·»åŠ å®Œæˆ")
    return df

# ==================== æ¨¡å‹è®­ç»ƒ ====================

def prepare_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
    """å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡"""
    
    # é€‰æ‹©æ‰€æœ‰æ•°å€¼ç‰¹å¾ï¼ˆæ’é™¤IDå’ŒåŸå§‹è®¡æ•°å’Œç›®æ ‡å˜é‡ï¼‰
    exclude_cols = {
        'video_id', 'title', 'channel_name', 'channel_id', 
        'thumbnail', 'published_date', 'view_count', 
        'like_count', 'comment_count', 'engagement_rate',
        'like_rate', 'comment_rate', 'log_engagement'  # æ·»åŠ log_engagementåˆ°æ’é™¤åˆ—è¡¨
    }
    
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    # ç§»é™¤åŒ…å«NaNçš„è¡Œ
    df_clean = df[feature_cols + ['log_engagement']].dropna()
    
    X = df_clean[feature_cols]
    y = df_clean['log_engagement']
    
    print(f"\nç‰¹å¾å‡†å¤‡å®Œæˆ:")
    print(f"   æ ·æœ¬æ•°: {len(df_clean)}")
    print(f"   ç‰¹å¾æ•°: {len(feature_cols)}")
    print(f"   ç‰¹å¾åˆ—è¡¨: {feature_cols}")
    
    return X, y, feature_cols

def train_model(X: pd.DataFrame, y: pd.Series, test_size: float = 0.2) -> Tuple[xgb.XGBRegressor, np.ndarray, np.ndarray]:
    """è®­ç»ƒXGBoostæ¨¡å‹"""
    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # è®­ç»ƒXGBoost
    print("\nè®­ç»ƒXGBoostæ¨¡å‹...")
    model = xgb.XGBRegressor(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        tree_method='hist'
    )
    
    model.fit(
        X_train_scaled, y_train,
        eval_set=[(X_test_scaled, y_test)],
        early_stopping_rounds=10,
        verbose=False
    )
    
    # è¯„ä¼°
    y_pred = model.predict(X_test_scaled)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    
    print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ:")
    print(f"   RÂ² Score: {r2:.4f}")
    print(f"   MAE: {mae:.4f}")
    print(f"   RMSE: {rmse:.4f}")
    
    return model, scaler, X_test_scaled, y_test, X_test

# ==================== ç‰¹å¾é‡è¦æ€§åˆ†æ ====================

def analyze_feature_importance(model: xgb.XGBRegressor, X_test: pd.DataFrame, 
                               y_test: pd.Series, feature_cols: List[str]):
    """ä½¿ç”¨SHAPåˆ†æç‰¹å¾è´¡çŒ®åº¦"""
    
    print("\nSHAPç‰¹å¾é‡è¦æ€§åˆ†æ...")
    
    # è®¡ç®—SHAPå€¼ï¼ˆä½¿ç”¨é‡‡æ ·ä»¥åŠ å¿«é€Ÿåº¦ï¼‰
    try:
        explainer = shap.TreeExplainer(model)
        # åªä½¿ç”¨å‰1000ä¸ªæ ·æœ¬ä»¥é¿å…å†…å­˜é—®é¢˜
        sample_size = min(1000, len(X_test))
        X_sample = X_test.iloc[:sample_size].copy()
        X_sample.columns = range(len(X_sample.columns))  # é‡å‘½ååˆ—ä¸ºæ•°å­—ï¼Œé¿å…é‡å¤
        shap_values = explainer.shap_values(X_sample)
        
        # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values, X_sample, plot_type="bar", show=False)
        plt.title("SHAPç‰¹å¾é‡è¦æ€§æ’å\nï¼ˆå¯¹äº’åŠ¨ç‡é¢„æµ‹çš„å¹³å‡å½±å“ï¼‰", fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig('feature_importance_shap.png', dpi=300, bbox_inches='tight')
        print(f"ç‰¹å¾é‡è¦æ€§å›¾è¡¨å·²ä¿å­˜: feature_importance_shap.png")
        plt.close()
        
        # ç»˜åˆ¶SHAPåŠ›å›¾ï¼ˆç¤ºä¾‹ï¼‰
        plt.figure(figsize=(12, 6))
        shap.summary_plot(shap_values, X_sample, show=False)
        plt.title("SHAPæ‘˜è¦å›¾", fontsize=12, fontweight='bold')
        plt.tight_layout()
        plt.savefig('shap_summary_plot.png', dpi=300, bbox_inches='tight')
        print(f"SHAPæ‘˜è¦å·²ä¿å­˜: shap_summary_plot.png")
        plt.close()
    except Exception as e:
        print(f"SHAPåˆ†æå¤±è´¥: {str(e)[:100]}")
        shap_values = None
        explainer = None
    
    return shap_values, explainer

# ==================== å®æ—¶æ‰“åˆ†å·¥å…· ====================

def score_single_title(title: str, model: xgb.XGBRegressor, 
                       scaler, feature_cols: List[str]) -> Dict:
    """ä¸ºå•ä¸ªæ ‡é¢˜æ‰“åˆ†"""
    
    # æå–ç‰¹å¾
    df_single = pd.DataFrame({'title': [title]})
    df_single = extract_text_features(df_single)
    
    # æ¨¡æ‹ŸDeepSeekè¯„åˆ†ï¼ˆå®é™…åº”è°ƒç”¨APIï¼‰
    df_single['creativity_score'] = np.random.uniform(4, 9, 1)
    df_single['emotional_appeal_score'] = np.random.uniform(3, 9, 1)
    df_single['clarity_score'] = np.random.uniform(5, 10, 1)
    
    # å‡†å¤‡ç‰¹å¾ï¼ˆä½¿ç”¨ç›¸åŒçš„feature_colsï¼‰
    X_single = df_single[feature_cols]
    X_single_scaled = scaler.transform(X_single)
    
    # é¢„æµ‹
    log_engagement_pred = model.predict(X_single_scaled)[0]
    engagement_rate_pred = (np.exp(log_engagement_pred) - 1) / 1000
    
    # æå–ç‰¹å¾å€¼ç”¨äºè§£é‡Š
    feature_values = X_single.iloc[0].to_dict()
    
    return {
        'title': title,
        'predicted_engagement_rate': max(0, engagement_rate_pred),
        'predicted_log_engagement': log_engagement_pred,
        'feature_values': feature_values,
        'creativity_score': df_single['creativity_score'].values[0],
        'emotional_appeal': df_single['emotional_appeal_score'].values[0]
    }

# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»æµç¨‹"""
    
    print("=" * 60)
    print("YouTubeæ ‡é¢˜è´¨é‡æ‰“åˆ†å™¨ v1.0")
    print("=" * 60)
    
    # 1. åŠ è½½æ•°æ®
    df = load_data(DATA_PATH)
    
    # 2. è®¡ç®—äº’åŠ¨æŒ‡æ ‡
    df = calculate_engagement_metrics(df)
    print(f"\näº’åŠ¨æŒ‡æ ‡ç»Ÿè®¡:")
    print(f"   å¹³å‡äº’åŠ¨ç‡: {df['engagement_rate'].mean():.6f}")
    print(f"   ä¸­ä½æ•°: {df['engagement_rate'].median():.6f}")
    
    # 3. æå–æ–‡æœ¬ç‰¹å¾
    df = extract_text_features(df)
    
    # 4. æ·»åŠ DeepSeekè¯„åˆ†
    df = add_deepseek_features(df, sample_size=500)
    
    # 5. å‡†å¤‡ç‰¹å¾
    X, y, feature_cols = prepare_features(df)
    
    # 6. è®­ç»ƒæ¨¡å‹
    model, scaler, X_test_scaled, y_test, X_test = train_model(X, y)
    
    # 7. ç‰¹å¾é‡è¦æ€§åˆ†æ
    shap_values, explainer = analyze_feature_importance(model, X_test, y_test, feature_cols)
    
    # 8. ç¤ºä¾‹æ‰“åˆ†
    print("\n" + "=" * 60)
    print("å®æ—¶æ‰“åˆ†ç¤ºä¾‹")
    print("=" * 60)
    
    test_titles = [
        "Why do Human Feet Wash up on This Beach? | Fascinating Horror Shorts",
        "The ULTIMATE iPhone 15 Review - You NEED to Watch This!",
        "Cooking Tutorial #256",
        "ğŸ”¥ SHOCKING Truth About AI That They Don't Want You to Know!!!",
    ]
    
    for title in test_titles:
        result = score_single_title(title, model, scaler, feature_cols)
        print(f"\n æ ‡é¢˜: {title[:50]}...")
        print(f"   é¢„æµ‹äº’åŠ¨ç‡: {result['predicted_engagement_rate']:.4f}")
        print(f"   åˆ›é€ åŠ›è¯„åˆ†: {result['creativity_score']:.1f}/10")
        print(f"   æƒ…æ„Ÿå¸å¼•åŠ›: {result['emotional_appeal']:.1f}/10")
    
    print("\n" + "=" * 60)
    print("  ç”Ÿæˆçš„å›¾è¡¨:")
    print("      - feature_importance_shap.png")
    print("      - shap_summary_plot.png")
    print("=" * 60)
    
    return model, scaler, feature_cols, df

if __name__ == "__main__":
    model, scaler, feature_cols, df = main()
